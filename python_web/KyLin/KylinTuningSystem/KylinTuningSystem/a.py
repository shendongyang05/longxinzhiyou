import numpy as np
import tensorflow as tf
import random
from collections import deque

class DeepRLScheduler:
    def __init__(self, machines_count, max_tasks, state_dim, action_dim):
        self.machines_count = machines_count  # æœºå™¨æ•°é‡
        self.max_tasks = max_tasks  # æœ€å¤§ä»»åŠ¡æ•°
        self.state_dim = state_dim  # çŠ¶æ€ç©ºé—´ç»´åº¦
        self.action_dim = action_dim  # åŠ¨ä½œç©ºé—´ç»´åº¦
        
        # ç»éªŒå›æ”¾ç¼“å†²åŒº
        self.memory = deque(maxlen=10000)
        
        # åˆ›å»ºActorå’ŒCriticç½‘ç»œ
        self.actor = self._build_actor()
        self.critic = self._build_critic()
        
        # ç›®æ ‡ç½‘ç»œ
        self.target_actor = self._build_actor()
        self.target_critic = self._build_critic()
        
        # åˆå§‹åŒ–ç›®æ ‡ç½‘ç»œæƒé‡
        self.target_actor.set_weights(self.actor.get_weights())
        self.target_critic.set_weights(self.critic.get_weights())
        
        # è¶…å‚æ•°
        self.gamma = 0.99  # æŠ˜æ‰£å› å­
        self.tau = 0.001   # è½¯æ›´æ–°å‚æ•°
        self.batch_size = 64
        
    def _build_actor(self):
        """æ„å»ºActorç½‘ç»œï¼Œç”¨äºç”ŸæˆåŠ¨ä½œ"""
        inputs = tf.keras.layers.Input(shape=(self.state_dim,))
        x = tf.keras.layers.Dense(256, activation='relu')(inputs)
        x = tf.keras.layers.Dense(128, activation='relu')(x)
        outputs = tf.keras.layers.Dense(self.action_dim, activation='softmax')(x)
        model = tf.keras.Model(inputs=inputs, outputs=outputs)
        model.compile(optimizer=tf.keras.optimizers.Adam(0.001))
        return model
    
    def _build_critic(self):
        """æ„å»ºCriticç½‘ç»œï¼Œç”¨äºè¯„ä¼°åŠ¨ä½œä»·å€¼"""
        state_input = tf.keras.layers.Input(shape=(self.state_dim,))
        action_input = tf.keras.layers.Input(shape=(self.action_dim,))
        
        x = tf.keras.layers.Concatenate()([state_input, action_input])
        x = tf.keras.layers.Dense(256, activation='relu')(x)
        x = tf.keras.layers.Dense(128, activation='relu')(x)
        outputs = tf.keras.layers.Dense(1)(x)
        
        model = tf.keras.Model(inputs=[state_input, action_input], outputs=outputs)
        model.compile(optimizer=tf.keras.optimizers.Adam(0.002), loss='mse')
        return model
    
    def get_state(self, machines, waiting_tasks):
        """æ„é€ ç³»ç»Ÿå½“å‰çŠ¶æ€
        åŒ…æ‹¬ï¼šæœºå™¨è¿è¡ŒçŠ¶æ€ã€ç­‰å¾…ä»»åŠ¡çŠ¶æ€ã€ç§¯å‹é˜Ÿåˆ—çŠ¶æ€ç­‰
        """
        state = []
        
        # æ·»åŠ æœºå™¨çŠ¶æ€ä¿¡æ¯
        for machine in machines:
            # å½’ä¸€åŒ–çš„CPUåˆ©ç”¨ç‡
            state.append(machine.cpu_usage / 100.0)
            # å½’ä¸€åŒ–çš„å†…å­˜åˆ©ç”¨ç‡
            state.append(machine.memory_usage / 100.0)
        
        # æ·»åŠ ç­‰å¾…ä»»åŠ¡ä¿¡æ¯
        for task in waiting_tasks:
            # å½’ä¸€åŒ–çš„ä»»åŠ¡CPUéœ€æ±‚
            state.append(task.cpu_demand / 100.0)
            # å½’ä¸€åŒ–çš„ä»»åŠ¡å†…å­˜éœ€æ±‚
            state.append(task.memory_demand / 100.0)
            # å½’ä¸€åŒ–çš„ä»»åŠ¡é¢„è®¡æ‰§è¡Œæ—¶é—´
            state.append(task.estimated_duration / 3600.0)
        
        # å¦‚æœç­‰å¾…ä»»åŠ¡ä¸è¶³ï¼Œç”¨0å¡«å……
        padding = self.max_tasks - len(waiting_tasks)
        state.extend([0] * (padding * 3))
        
        return np.array(state)
    
    def select_action(self, state):
        """æ ¹æ®å½“å‰çŠ¶æ€é€‰æ‹©åŠ¨ä½œ"""
        action_probs = self.actor.predict(np.array([state]))[0]
        return np.argmax(action_probs)
    
    def calculate_reward(self, completed_tasks, current_time):
        """è®¡ç®—å¥–åŠ±å‡½æ•°
        r = -âˆ‘(t/d_j)ï¼Œå…¶ä¸­tä¸ºä¸¤æ¬¡è°ƒåº¦é—´éš”ï¼Œd_jä¸ºä»»åŠ¡jçš„å®é™…æ‰§è¡Œæ—¶é—´
        """
        total_weighted_turnaround = 0
        
        for task in completed_tasks:
            # è®¡ç®—ä»»åŠ¡çš„å¸¦æƒå‘¨è½¬æ—¶é—´
            turnaround_time = task.completion_time - task.arrival_time
            weighted_turnaround = turnaround_time / task.execution_time
            total_weighted_turnaround += weighted_turnaround
        
        # è´Ÿå€¼ä½œä¸ºå¥–åŠ±ï¼Œå› ä¸ºæˆ‘ä»¬è¦æœ€å°åŒ–å¸¦æƒå‘¨è½¬æ—¶é—´
        reward = -total_weighted_turnaround
        return reward
    
    def remember(self, state, action, reward, next_state, done):
        """å­˜å‚¨ç»éªŒåˆ°å›æ”¾ç¼“å†²åŒº"""
        self.memory.append((state, action, reward, next_state, done))
    
    def replay(self):
        """ä»ç»éªŒå›æ”¾ç¼“å†²åŒºä¸­é‡‡æ ·å¹¶è®­ç»ƒç½‘ç»œ"""
        if len(self.memory) < self.batch_size:
            return
        
        # éšæœºé‡‡æ ·
        minibatch = random.sample(self.memory, self.batch_size)
        
        states = np.array([experience[0] for experience in minibatch])
        actions = np.array([experience[1] for experience in minibatch])
        rewards = np.array([experience[2] for experience in minibatch])
        next_states = np.array([experience[3] for experience in minibatch])
        dones = np.array([experience[4] for experience in minibatch])
        
        # å°†åŠ¨ä½œè½¬æ¢ä¸ºone-hotç¼–ç 
        actions_one_hot = tf.one_hot(actions, self.action_dim)
        
        # æ›´æ–°Critic
        target_actions = self.target_actor.predict(next_states)
        target_q_values = self.target_critic.predict([next_states, target_actions])
        
        y = rewards + self.gamma * target_q_values * (1 - dones)
        self.critic.fit([tf.convert_to_tensor(states, dtype=tf.float32), actions_one_hot], y, verbose=0)
        
        # æ›´æ–°Actor
        with tf.GradientTape() as tape:
            actions_pred = self.actor(tf.convert_to_tensor(states, dtype=tf.float32))
            critic_value = self.critic([tf.convert_to_tensor(states, dtype=tf.float32), actions_pred])
            actor_loss = -tf.math.reduce_mean(critic_value)
        
        actor_gradients = tape.gradient(actor_loss, self.actor.trainable_variables)
        self.actor.optimizer.apply_gradients(zip(actor_gradients, self.actor.trainable_variables))
        
        # è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ
        self._update_target_networks()
    
    def _update_target_networks(self):
        """è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ"""
        actor_weights = self.actor.get_weights()
        target_actor_weights = self.target_actor.get_weights()
        critic_weights = self.critic.get_weights()
        target_critic_weights = self.target_critic.get_weights()
        
        for i in range(len(actor_weights)):
            target_actor_weights[i] = self.tau * actor_weights[i] + (1 - self.tau) * target_actor_weights[i]
        
        for i in range(len(critic_weights)):
            target_critic_weights[i] = self.tau * critic_weights[i] + (1 - self.tau) * target_critic_weights[i]
        
        self.target_actor.set_weights(target_actor_weights)
        self.target_critic.set_weights(target_critic_weights)


class Task:
    def __init__(self, task_id, arrival_time, cpu_demand, memory_demand, estimated_duration):
        self.task_id = task_id
        self.arrival_time = arrival_time
        self.cpu_demand = cpu_demand
        self.memory_demand = memory_demand
        self.estimated_duration = estimated_duration
        self.execution_time = None  # å®é™…æ‰§è¡Œæ—¶é—´ï¼Œè°ƒåº¦åç¡®å®š
        self.completion_time = None  # å®Œæˆæ—¶é—´
        self.assigned_machine = None  # åˆ†é…çš„æœºå™¨


class Machine:
    def __init__(self, machine_id, cpu_capacity, memory_capacity):
        self.machine_id = machine_id
        self.cpu_capacity = cpu_capacity
        self.memory_capacity = memory_capacity
        self.cpu_usage = 0
        self.memory_usage = 0
        self.running_tasks = []
    
    def can_accommodate(self, task):
        """æ£€æŸ¥æœºå™¨æ˜¯å¦èƒ½å®¹çº³ä»»åŠ¡"""
        return (self.cpu_usage + task.cpu_demand <= self.cpu_capacity and
                self.memory_usage + task.memory_demand <= self.memory_capacity)
    
    def assign_task(self, task, current_time):
        """åˆ†é…ä»»åŠ¡åˆ°æœºå™¨"""
        if self.can_accommodate(task):
            self.running_tasks.append(task)
            self.cpu_usage += task.cpu_demand
            self.memory_usage += task.memory_demand
            task.assigned_machine = self
            # è®¾ç½®å®é™…æ‰§è¡Œæ—¶é—´ï¼ˆå¯ä»¥åŠ å…¥ä¸€äº›éšæœºæ€§æ¨¡æ‹Ÿå®é™…ç¯å¢ƒï¼‰
            task.execution_time = task.estimated_duration * (0.9 + 0.2 * random.random())
            task.completion_time = current_time + task.execution_time
            return True
        return False
    
    def update(self, current_time):
        """æ›´æ–°æœºå™¨çŠ¶æ€ï¼Œç§»é™¤å·²å®Œæˆçš„ä»»åŠ¡"""
        completed_tasks = []
        remaining_tasks = []
        
        for task in self.running_tasks:
            if current_time >= task.completion_time:
                completed_tasks.append(task)
                self.cpu_usage -= task.cpu_demand
                self.memory_usage -= task.memory_demand
            else:
                remaining_tasks.append(task)
        
        self.running_tasks = remaining_tasks
        return completed_tasks


class Simulator:
    def __init__(self, num_machines, num_tasks):
        self.current_time = 0
        self.machines = [Machine(i, 100, 100) for i in range(num_machines)]
        self.waiting_tasks = deque()
        self.completed_tasks = []
        
        # åˆ›å»ºä»»åŠ¡
        for i in range(num_tasks):
            arrival_time = i * 10  # ç®€åŒ–ï¼šæ¯10ä¸ªæ—¶é—´å•ä½åˆ°è¾¾ä¸€ä¸ªä»»åŠ¡
            cpu_demand = random.randint(10, 50)
            memory_demand = random.randint(10, 50)
            estimated_duration = random.randint(20, 200)
            task = Task(i, arrival_time, cpu_demand, memory_demand, estimated_duration)
            self.waiting_tasks.append(task)
        
        # åˆå§‹åŒ–è°ƒåº¦å™¨
        state_dim = num_machines * 2 + num_tasks * 3  # æœºå™¨çŠ¶æ€ + ä»»åŠ¡çŠ¶æ€
        action_dim = num_machines * num_tasks  # ç®€åŒ–ï¼šæ¯ä¸ªåŠ¨ä½œå¯¹åº”å°†ä¸€ä¸ªä»»åŠ¡åˆ†é…åˆ°ä¸€ä¸ªæœºå™¨
        self.scheduler = DeepRLScheduler(num_machines, num_tasks, state_dim, action_dim)
    
    def run_fcfs(self):
        """è¿è¡Œå…ˆæ¥å…ˆæœåŠ¡(FCFS)ç®—æ³•è¿›è¡Œå¯¹æ¯”"""
        self.current_time = 0
        self.completed_tasks = []
        
        # å¤åˆ¶ç­‰å¾…ä»»åŠ¡é˜Ÿåˆ—
        waiting_tasks = deque(self.waiting_tasks)
        
        while waiting_tasks or any(machine.running_tasks for machine in self.machines):
            # æ›´æ–°æœºå™¨çŠ¶æ€
            for machine in self.machines:
                completed = machine.update(self.current_time)
                self.completed_tasks.extend(completed)
            
            # æ£€æŸ¥æ–°åˆ°è¾¾çš„ä»»åŠ¡
            while waiting_tasks and waiting_tasks[0].arrival_time <= self.current_time:
                task = waiting_tasks.popleft()
                
                # å°è¯•åˆ†é…åˆ°ç¬¬ä¸€ä¸ªå¯ç”¨çš„æœºå™¨
                assigned = False
                for machine in self.machines:
                    if machine.can_accommodate(task):
                        machine.assign_task(task, self.current_time)
                        assigned = True
                        break
                
                # å¦‚æœæ— æ³•åˆ†é…ï¼Œæ”¾å›é˜Ÿåˆ—å¤´éƒ¨
                if not assigned:
                    waiting_tasks.appendleft(task)
                    break
            
            # æ—¶é—´å‰è¿›
            self.current_time += 1
        
        # è®¡ç®—è¯„ä»·æŒ‡æ ‡
        avg_weighted_turnaround = self._calculate_avg_weighted_turnaround()
        makespan = self._calculate_makespan()
        
        return avg_weighted_turnaround, makespan
    
    def run_drl(self, episodes=100):
        """è¿è¡Œæ·±åº¦å¼ºåŒ–å­¦ä¹ è°ƒåº¦ç®—æ³•"""
        best_avg_weighted_turnaround = float('inf')
        best_makespan = float('inf')
        
        for episode in range(episodes):
            # é‡ç½®ç¯å¢ƒ
            self.current_time = 0
            self.completed_tasks = []
            waiting_tasks = deque(self.waiting_tasks)
            
            for machine in self.machines:
                machine.cpu_usage = 0
                machine.memory_usage = 0
                machine.running_tasks = []
            
            done = False
            episode_reward = 0
            
            while not done:
                # è·å–å½“å‰çŠ¶æ€
                state = self.scheduler.get_state(self.machines, list(waiting_tasks))
                
                # é€‰æ‹©åŠ¨ä½œ
                action = self.scheduler.select_action(state)
                
                # æ‰§è¡ŒåŠ¨ä½œ
                if len(waiting_tasks) > 0:
                    task_idx = action % len(waiting_tasks)
                    machine_idx = action // len(waiting_tasks)
                    
                    if task_idx < len(waiting_tasks) and machine_idx < len(self.machines):
                        task = list(waiting_tasks)[task_idx]
                        machine = self.machines[machine_idx]
                        
                        if task.arrival_time <= self.current_time and machine.can_accommodate(task):
                            # ä»ç­‰å¾…é˜Ÿåˆ—ä¸­ç§»é™¤ä»»åŠ¡
                            waiting_tasks.remove(task)
                            # åˆ†é…ä»»åŠ¡åˆ°æœºå™¨
                            machine.assign_task(task, self.current_time)
                
                # æ›´æ–°æ‰€æœ‰æœºå™¨çŠ¶æ€
                newly_completed = []
                for machine in self.machines:
                    completed = machine.update(self.current_time)
                    newly_completed.extend(completed)
                    self.completed_tasks.extend(completed)
                
                # è®¡ç®—å¥–åŠ±
                reward = self.scheduler.calculate_reward(newly_completed, self.current_time)
                episode_reward += reward
                
                # æ—¶é—´å‰è¿›
                self.current_time += 1
                
                # è·å–ä¸‹ä¸€ä¸ªçŠ¶æ€
                next_state = self.scheduler.get_state(self.machines, list(waiting_tasks))
                
                # æ£€æŸ¥æ˜¯å¦ç»“æŸ
                done = (len(waiting_tasks) == 0 and 
                        all(len(machine.running_tasks) == 0 for machine in self.machines))
                
                # å­˜å‚¨ç»éªŒ
                self.scheduler.remember(state, action, reward, next_state, done)
                
                # è®­ç»ƒç½‘ç»œ
                self.scheduler.replay()
            
            # è®¡ç®—è¯„ä»·æŒ‡æ ‡
            avg_weighted_turnaround = self._calculate_avg_weighted_turnaround()
            makespan = self._calculate_makespan()
            
            # æ›´æ–°æœ€ä½³ç»“æœ
            if avg_weighted_turnaround < best_avg_weighted_turnaround:
                best_avg_weighted_turnaround = avg_weighted_turnaround
            
            if makespan < best_makespan:
                best_makespan = makespan
            
            # æ˜¾ç¤ºè®­ç»ƒè¿›åº¦
            if (episode + 1) % 10 == 0:
                print(f"E{episode+1:2d}", end=" ", flush=True)
        
        return best_avg_weighted_turnaround, best_makespan
    
    def _calculate_avg_weighted_turnaround(self):
        """è®¡ç®—å¹³å‡å¸¦æƒå‘¨è½¬æ—¶é—´"""
        if not self.completed_tasks:
            return 0
        
        total_weighted_turnaround = 0
        for task in self.completed_tasks:
            turnaround_time = task.completion_time - task.arrival_time
            weighted_turnaround = turnaround_time / task.execution_time
            total_weighted_turnaround += weighted_turnaround
        
        return total_weighted_turnaround / len(self.completed_tasks)
    
    def _calculate_makespan(self):
        """è®¡ç®—å®Œå·¥æ—¶é—´"""
        if not self.completed_tasks:
            return 0
        
        return max(task.completion_time for task in self.completed_tasks)


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    print("=" * 60)
    print("ğŸš€ æ·±åº¦å¼ºåŒ–å­¦ä¹ ä»»åŠ¡è°ƒåº¦ç³»ç»Ÿæ€§èƒ½å¯¹æ¯”")
    print("=" * 60)
    
    # åˆ›å»ºæ¨¡æ‹Ÿç¯å¢ƒï¼š10å°æœºå™¨ï¼Œ100ä¸ªä»»åŠ¡
    print("ğŸ“Š æ¨¡æ‹Ÿç¯å¢ƒé…ç½®:")
    print("   - æœºå™¨æ•°é‡: 10å°")
    print("   - ä»»åŠ¡æ•°é‡: 100ä¸ª")
    print("   - è®­ç»ƒè½®æ•°: 20è½®")
    print()
    
    sim = Simulator(10, 100)
    
    print("ğŸ”„ æ­£åœ¨è¿è¡ŒFCFSï¼ˆå…ˆæ¥å…ˆæœåŠ¡ï¼‰ç®—æ³•...")
    fcfs_wt, fcfs_makespan = sim.run_fcfs()
    print("âœ… FCFSç®—æ³•å®Œæˆ")
    print()
    
    print("ğŸ§  æ­£åœ¨è®­ç»ƒæ·±åº¦å¼ºåŒ–å­¦ä¹ è°ƒåº¦å™¨...")
    print("   è®­ç»ƒè¿›åº¦: ", end="", flush=True)
    drl_wt, drl_makespan = sim.run_drl(episodes=20)
    print("\nâœ… æ·±åº¦å¼ºåŒ–å­¦ä¹ è®­ç»ƒå®Œæˆ")
    print()
    
    # è®¡ç®—æ”¹è¿›ç™¾åˆ†æ¯”
    wt_improvement = (fcfs_wt - drl_wt) / fcfs_wt * 100
    makespan_improvement = (fcfs_makespan - drl_makespan) / fcfs_makespan * 100
    
    print("ğŸ“ˆ æ€§èƒ½å¯¹æ¯”ç»“æœ:")
    print("=" * 60)
    print(f"{'æŒ‡æ ‡':<20} {'FCFS':<15} {'DRL':<15} {'æ”¹è¿›':<10}")
    print("-" * 60)
    print(f"{'å¹³å‡å¸¦æƒå‘¨è½¬æ—¶é—´':<20} {fcfs_wt:<15.2f} {drl_wt:<15.2f} {wt_improvement:>+.2f}%")
    print(f"{'å®Œå·¥æ—¶é—´':<20} {fcfs_makespan:<15.2f} {drl_makespan:<15.2f} {makespan_improvement:>+.2f}%")
    print("=" * 60)
    
    print("\nğŸ¯ æ•ˆç‡æå‡æ€»ç»“:")
    if wt_improvement > 0:
        print(f"   âœ… å¸¦æƒå‘¨è½¬æ—¶é—´é™ä½äº† {wt_improvement:.2f}%")
    else:
        print(f"   âŒ å¸¦æƒå‘¨è½¬æ—¶é—´å¢åŠ äº† {abs(wt_improvement):.2f}%")
    
    if makespan_improvement > 0:
        print(f"   âœ… å®Œå·¥æ—¶é—´é™ä½äº† {makespan_improvement:.2f}%")
    else:
        print(f"   âŒ å®Œå·¥æ—¶é—´å¢åŠ äº† {abs(makespan_improvement):.2f}%")
    
    overall_improvement = (wt_improvement + makespan_improvement) / 2
    print(f"\nğŸ“Š ç»¼åˆæ€§èƒ½æå‡: {overall_improvement:+.2f}%")
    
    if overall_improvement > 0:
        print("ğŸ‰ æ·±åº¦å¼ºåŒ–å­¦ä¹ è°ƒåº¦å™¨è¡¨ç°ä¼˜å¼‚ï¼")
    else:
        print("âš ï¸  éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–è®­ç»ƒå‚æ•°")
    
    print("\n" + "=" * 60)
